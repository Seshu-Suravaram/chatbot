{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85cb7e-fe27-443e-8482-52fd32e1c925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame with specified data types\n",
    "df = pd.read_csv('twcs.csv', dtype={\n",
    "    'tweet_id': int,\n",
    "    'author_id': str,\n",
    "    'inbound': bool,\n",
    "    'created_at': str,\n",
    "    'text': str,\n",
    "    'response_tweet_id': str,\n",
    "    'in_response_to_tweet_id': str\n",
    "})\n",
    "\n",
    "# Display the first 10 rows as a table\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f028a52a-f2f8-40bf-ae3d-0b3f6477c32b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author_id\n",
      "AmazonHelp        169840\n",
      "AppleSupport      106860\n",
      "Uber_Support       56270\n",
      "SpotifyCares       43265\n",
      "Delta              42253\n",
      "                   ...  \n",
      "JackBox              266\n",
      "OfficeSupport        218\n",
      "AskDSC               210\n",
      "CarlsJr              196\n",
      "HotelTonightCX       152\n",
      "Name: count, Length: 108, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter customer care accounts (alphanumeric author_ids)\n",
    "customer_care_accounts_all = df[pd.to_numeric(df['author_id'], errors='coerce').isna()]['author_id']\n",
    "\n",
    "# Count tweets for each customer care account\n",
    "tweet_counts = customer_care_accounts_all.value_counts()\n",
    "\n",
    "# Display the results in descending order\n",
    "print(tweet_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d91f76e-b2ac-464e-b2a7-f7668028075b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprintcare.csv: 48263 rows\n",
      "Ask_Spectrum.csv: 57474 rows\n",
      "VerizonSupport.csv: 41472 rows\n",
      "ChipotleTweets.csv: 41612 rows\n",
      "AskPlayStation.csv: 42795 rows\n",
      "marksandspencer.csv: 26061 rows\n",
      "MicrosoftHelps.csv: 23427 rows\n",
      "ATVIAssist.csv: 47994 rows\n",
      "AdobeCare.csv: 20289 rows\n",
      "AmazonHelp.csv: 367944 rows\n",
      "XboxSupport.csv: 55383 rows\n",
      "AirbnbHelp.csv: 19020 rows\n",
      "nationalrailenq.csv: 10734 rows\n",
      "AirAsiaSupport.csv: 26593 rows\n",
      "Morrisons.csv: 22186 rows\n",
      "NikeSupport.csv: 7709 rows\n",
      "AskAmex.csv: 20867 rows\n",
      "McDonalds.csv: 19951 rows\n",
      "YahooCare.csv: 2145 rows\n",
      "AskLyft.csv: 26000 rows\n",
      "UPSHelp.csv: 41470 rows\n",
      "Delta.csv: 89263 rows\n",
      "AppleSupport.csv: 235215 rows\n",
      "Uber_Support.csv: 126019 rows\n",
      "Tesco.csv: 74350 rows\n",
      "SpotifyCares.csv: 90459 rows\n",
      "British_Airways.csv: 60777 rows\n",
      "comcastcares.csv: 69773 rows\n",
      "AmericanAir.csv: 87768 rows\n",
      "TMobileHelp.csv: 74965 rows\n",
      "VirginTrains.csv: 65789 rows\n",
      "SouthwestAir.csv: 65514 rows\n",
      "AskeBay.csv: 20593 rows\n",
      "hulu_support.csv: 47528 rows\n",
      "GWRHelp.csv: 46923 rows\n",
      "sainsburys.csv: 43684 rows\n",
      "AskPayPal.csv: 25801 rows\n",
      "HPSupport.csv: 12536 rows\n",
      "ChaseSupport.csv: 18137 rows\n",
      "CoxHelp.csv: 17008 rows\n",
      "DropboxSupport.csv: 11265 rows\n",
      "VirginAtlantic.csv: 10467 rows\n",
      "BofA_Help.csv: 23988 rows\n",
      "AzureSupport.csv: 14619 rows\n",
      "AlaskaAir.csv: 17168 rows\n",
      "ArgosHelpers.csv: 27476 rows\n",
      "Postmates_Help.csv: 7598 rows\n",
      "AskTarget.csv: 29912 rows\n",
      "GoDaddyHelp.csv: 4351 rows\n",
      "CenturyLinkHelp.csv: 8328 rows\n",
      "AskPapaJohns.csv: 8710 rows\n",
      "SW_Help.csv: 28502 rows\n",
      "askpanera.csv: 4760 rows\n",
      "Walmart.csv: 6089 rows\n",
      "USCellularCares.csv: 2549 rows\n",
      "AsurionCares.csv: 4242 rows\n",
      "GloCare.csv: 14928 rows\n",
      "idea_cares.csv: 34110 rows\n",
      "DoorDash_Help.csv: 3673 rows\n",
      "NeweggService.csv: 2294 rows\n",
      "VirginAmerica.csv: 7356 rows\n",
      "Safaricom_Care.csv: 33553 rows\n",
      "DunkinDonuts.csv: 2914 rows\n",
      "Ask_WellsFargo.csv: 15834 rows\n",
      "O2.csv: 36166 rows\n",
      "TfL.csv: 5378 rows\n",
      "asksalesforce.csv: 1610 rows\n",
      "airtel_care.csv: 17849 rows\n",
      "Kimpton.csv: 2870 rows\n",
      "AskCiti.csv: 7692 rows\n",
      "IHGService.csv: 5326 rows\n",
      "LondonMidland.csv: 16738 rows\n",
      "JetBlue.csv: 18520 rows\n",
      "BoostCare.csv: 1998 rows\n",
      "JackBox.csv: 588 rows\n",
      "AldiUK.csv: 19315 rows\n",
      "HiltonHelp.csv: 1815 rows\n",
      "GooglePlayMusic.csv: 1944 rows\n",
      "OfficeSupport.csv: 535 rows\n",
      "KFC_UKI_Help.csv: 4967 rows\n",
      "DellCares.csv: 9409 rows\n",
      "TwitterSupport.csv: 3503 rows\n",
      "GreggsOfficial.csv: 10852 rows\n",
      "ATT.csv: 10631 rows\n",
      "TacoBellTeam.csv: 8882 rows\n",
      "AskRBC.csv: 2342 rows\n",
      "ArbysCares.csv: 4095 rows\n",
      "NortonSupport.csv: 4022 rows\n",
      "AskSeagate.csv: 1348 rows\n",
      "sizehelpteam.csv: 3522 rows\n",
      "SCsupport.csv: 2936 rows\n",
      "MOO.csv: 1422 rows\n",
      "AskDSC.csv: 439 rows\n",
      "AskVirginMoney.csv: 1898 rows\n",
      "AskRobinhood.csv: 996 rows\n",
      "MTNC_Care.csv: 1871 rows\n",
      "AWSSupport.csv: 2394 rows\n",
      "VMUcare.csv: 3035 rows\n",
      "mediatemplehelp.csv: 601 rows\n",
      "AskTigogh.csv: 1518 rows\n",
      "PandoraSupport.csv: 2277 rows\n",
      "askvisa.csv: 1377 rows\n",
      "OPPOCareIN.csv: 1885 rows\n",
      "ask_progressive.csv: 1341 rows\n",
      "PearsonSupport.csv: 1804 rows\n",
      "CarlsJr.csv: 446 rows\n",
      "HotelTonightCX.csv: 325 rows\n",
      "KeyBank_Help.csv: 1212 rows\n",
      "Total files generated: 108\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# This codeblock will split twcs.csv into separate file per \n",
    "# customer care account. new field `text_nomentions` has all \n",
    "# the twitter mentions removed.\n",
    "#\n",
    "# output location:\n",
    "#        /twcs/by_account/   \n",
    "###########################################################\n",
    "\n",
    "# Function to extract tweet IDs from comma-separated strings (handling NaN gracefully)\n",
    "def extract_tweet_ids(id_str):\n",
    "    if pd.isna(id_str):\n",
    "        return []\n",
    "    elif isinstance(id_str, str):  # Check if it's a string\n",
    "        return [int(x) for x in id_str.split(',') if x.strip().isdigit()]\n",
    "    elif isinstance(id_str, list):  # Check if it's already a list\n",
    "        return id_str  # No need to process further\n",
    "    else:\n",
    "        return []  # Handle other unexpected data types\n",
    "\n",
    "# Apply the function to extract tweet IDs\n",
    "df['response_tweet_id_list'] = df['response_tweet_id'].apply(extract_tweet_ids)\n",
    "df['in_response_to_tweet_id_list'] = df['in_response_to_tweet_id'].apply(extract_tweet_ids)\n",
    "\n",
    "# Identify unique customer care accounts (alphanumeric author_ids)\n",
    "customer_care_accounts = df[pd.to_numeric(df['author_id'], errors='coerce').isna()]['author_id'].unique()\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = 'twcs/by_account/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each customer care account\n",
    "for account in customer_care_accounts:\n",
    "    # Filter tweets directly involving the account (author or mentioned in text)\n",
    "    account_tweets = df[\n",
    "        (df['author_id'] == account) | \n",
    "        (df['text'].str.contains(f\"@{account}\", case=False, na=False))\n",
    "    ]\n",
    "\n",
    "    # Get all tweet IDs related to the account (including responses and in_response_to)\n",
    "    related_tweet_ids = set(\n",
    "        account_tweets['tweet_id'].tolist() +\n",
    "        [item for sublist in account_tweets['response_tweet_id_list'] if isinstance(sublist, list) for item in sublist] + \n",
    "        [item for sublist in account_tweets['in_response_to_tweet_id_list'] if isinstance(sublist, list) for item in sublist]\n",
    "    )\n",
    "\n",
    "    # Filter all tweets with the related tweet IDs\n",
    "    all_conversation_tweets = df[df['tweet_id'].isin(related_tweet_ids)].copy()  # Create a copy here\n",
    "\n",
    "    # Remove @ mentions and trailing spaces from the 'text' field\n",
    "    all_conversation_tweets['text_nomentions'] = all_conversation_tweets['text'].astype(str).str.replace(r'@[^\\s]+', '', regex=True)\n",
    "\n",
    "    # Reorder columns to place 'text_nomentions' after 'text'\n",
    "    all_conversation_tweets = all_conversation_tweets[[\n",
    "        'tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'text_nomentions', \n",
    "        'response_tweet_id', 'in_response_to_tweet_id'\n",
    "    ]]\n",
    "\n",
    "    # Write the conversation tweets to a CSV file, overwriting if it already exists\n",
    "    output_file = os.path.join(output_dir, f\"{account}.csv\")\n",
    "    all_conversation_tweets.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print the number of rows in the output file\n",
    "    print(f\"{account}.csv: {len(all_conversation_tweets)} rows\")\n",
    "\n",
    "# Print the total number of files generated\n",
    "print(f\"Total files generated: {len(customer_care_accounts)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0afac3f3-5054-4068-90a1-6f136274dfb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 tweets by ChipotleTweets contain the word 'because' (1.09% of total tweets)\n",
      "210 tweets by GWRHelp contain the word 'because' (1.08% of total tweets)\n",
      "445 tweets by SpotifyCares contain the word 'because' (1.03% of total tweets)\n",
      "13 tweets by Kimpton contain the word 'because' (0.97% of total tweets)\n",
      "31 tweets by nationalrailenq contain the word 'because' (0.70% of total tweets)\n",
      "8 tweets by VMUcare contain the word 'because' (0.58% of total tweets)\n",
      "66 tweets by SW_Help contain the word 'because' (0.54% of total tweets)\n",
      "45 tweets by McDonalds contain the word 'because' (0.53% of total tweets)\n",
      "1 tweets by CarlsJr contain the word 'because' (0.51% of total tweets)\n",
      "22 tweets by VirginAtlantic contain the word 'because' (0.51% of total tweets)\n",
      "4 tweets by PearsonSupport contain the word 'because' (0.48% of total tweets)\n",
      "42 tweets by AskeBay contain the word 'because' (0.43% of total tweets)\n",
      "32 tweets by JetBlue contain the word 'because' (0.40% of total tweets)\n",
      "10 tweets by Walmart contain the word 'because' (0.38% of total tweets)\n",
      "8 tweets by TfL contain the word 'because' (0.36% of total tweets)\n",
      "22 tweets by GloCare contain the word 'because' (0.35% of total tweets)\n",
      "45 tweets by AskTarget contain the word 'because' (0.34% of total tweets)\n",
      "4 tweets by USCellularCares contain the word 'because' (0.34% of total tweets)\n",
      "23 tweets by LondonMidland contain the word 'because' (0.34% of total tweets)\n",
      "2 tweets by ask_progressive contain the word 'because' (0.33% of total tweets)\n",
      "4 tweets by SCsupport contain the word 'because' (0.30% of total tweets)\n",
      "36 tweets by ArgosHelpers contain the word 'because' (0.30% of total tweets)\n",
      "9 tweets by Postmates_Help contain the word 'because' (0.27% of total tweets)\n",
      "7 tweets by IHGService contain the word 'because' (0.26% of total tweets)\n",
      "76 tweets by SouthwestAir contain the word 'because' (0.26% of total tweets)\n",
      "101 tweets by Tesco contain the word 'because' (0.26% of total tweets)\n",
      "110 tweets by Delta contain the word 'because' (0.26% of total tweets)\n",
      "57 tweets by sprintcare contain the word 'because' (0.25% of total tweets)\n",
      "5 tweets by GoDaddyHelp contain the word 'because' (0.25% of total tweets)\n",
      "40 tweets by idea_cares contain the word 'because' (0.25% of total tweets)\n",
      "28 tweets by MicrosoftHelps contain the word 'because' (0.25% of total tweets)\n",
      "72 tweets by British_Airways contain the word 'because' (0.25% of total tweets)\n",
      "80 tweets by TMobileHelp contain the word 'because' (0.23% of total tweets)\n",
      "17 tweets by AlaskaAir contain the word 'because' (0.23% of total tweets)\n",
      "40 tweets by UPSHelp contain the word 'because' (0.22% of total tweets)\n",
      "2 tweets by YahooCare contain the word 'because' (0.22% of total tweets)\n",
      "12 tweets by DropboxSupport contain the word 'because' (0.20% of total tweets)\n",
      "2 tweets by PandoraSupport contain the word 'because' (0.19% of total tweets)\n",
      "15 tweets by CoxHelp contain the word 'because' (0.19% of total tweets)\n",
      "16 tweets by ChaseSupport contain the word 'because' (0.18% of total tweets)\n",
      "1 tweets by KeyBank_Help contain the word 'because' (0.18% of total tweets)\n",
      "5 tweets by ATT contain the word 'because' (0.18% of total tweets)\n",
      "64 tweets by AmericanAir contain the word 'because' (0.17% of total tweets)\n",
      "20 tweets by marksandspencer contain the word 'because' (0.17% of total tweets)\n",
      "46 tweets by VirginTrains contain the word 'because' (0.17% of total tweets)\n",
      "19 tweets by AskLyft contain the word 'because' (0.16% of total tweets)\n",
      "1 tweets by MOO contain the word 'because' (0.16% of total tweets)\n",
      "1 tweets by AskSeagate contain the word 'because' (0.16% of total tweets)\n",
      "6 tweets by AskCiti contain the word 'because' (0.15% of total tweets)\n",
      "4 tweets by VirginAmerica contain the word 'because' (0.14% of total tweets)\n",
      "16 tweets by AskPayPal contain the word 'because' (0.14% of total tweets)\n",
      "1 tweets by asksalesforce contain the word 'because' (0.14% of total tweets)\n",
      "1 tweets by AskTigogh contain the word 'because' (0.14% of total tweets)\n",
      "12 tweets by AirbnbHelp contain the word 'because' (0.14% of total tweets)\n",
      "24 tweets by VerizonSupport contain the word 'because' (0.13% of total tweets)\n",
      "9 tweets by HPSupport contain the word 'because' (0.12% of total tweets)\n",
      "24 tweets by sainsburys contain the word 'because' (0.12% of total tweets)\n",
      "14 tweets by AskAmex contain the word 'because' (0.12% of total tweets)\n",
      "12 tweets by AdobeCare contain the word 'because' (0.12% of total tweets)\n",
      "19 tweets by Safaricom_Care contain the word 'because' (0.12% of total tweets)\n",
      "30 tweets by Ask_Spectrum contain the word 'because' (0.12% of total tweets)\n",
      "6 tweets by DellCares contain the word 'because' (0.11% of total tweets)\n",
      "11 tweets by Morrisons contain the word 'because' (0.11% of total tweets)\n",
      "4 tweets by AskPapaJohns contain the word 'because' (0.10% of total tweets)\n",
      "2 tweets by NortonSupport contain the word 'because' (0.10% of total tweets)\n",
      "1 tweets by AskRBC contain the word 'because' (0.10% of total tweets)\n",
      "1 tweets by AWSSupport contain the word 'because' (0.10% of total tweets)\n",
      "2 tweets by askpanera contain the word 'because' (0.09% of total tweets)\n",
      "4 tweets by GreggsOfficial contain the word 'because' (0.09% of total tweets)\n",
      "3 tweets by CenturyLinkHelp contain the word 'because' (0.08% of total tweets)\n",
      "10 tweets by AirAsiaSupport contain the word 'because' (0.08% of total tweets)\n",
      "13 tweets by ATVIAssist contain the word 'because' (0.07% of total tweets)\n",
      "11 tweets by O2 contain the word 'because' (0.07% of total tweets)\n",
      "5 tweets by AldiUK contain the word 'because' (0.06% of total tweets)\n",
      "14 tweets by XboxSupport contain the word 'because' (0.06% of total tweets)\n",
      "95 tweets by AmazonHelp contain the word 'because' (0.06% of total tweets)\n",
      "10 tweets by hulu_support contain the word 'because' (0.05% of total tweets)\n",
      "45 tweets by AppleSupport contain the word 'because' (0.04% of total tweets)\n",
      "13 tweets by comcastcares contain the word 'because' (0.04% of total tweets)\n",
      "3 tweets by airtel_care contain the word 'because' (0.03% of total tweets)\n",
      "1 tweets by NikeSupport contain the word 'because' (0.03% of total tweets)\n",
      "2 tweets by Ask_WellsFargo contain the word 'because' (0.03% of total tweets)\n",
      "3 tweets by BofA_Help contain the word 'because' (0.02% of total tweets)\n",
      "1 tweets by AzureSupport contain the word 'because' (0.01% of total tweets)\n",
      "7 tweets by Uber_Support contain the word 'because' (0.01% of total tweets)\n",
      "2 tweets by AskPlayStation contain the word 'because' (0.01% of total tweets)\n",
      "0 tweets by AsurionCares contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by DoorDash_Help contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by NeweggService contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by DunkinDonuts contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by BoostCare contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by JackBox contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by HiltonHelp contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by GooglePlayMusic contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by OfficeSupport contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by KFC_UKI_Help contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by TwitterSupport contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by TacoBellTeam contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by ArbysCares contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by sizehelpteam contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by AskDSC contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by AskVirginMoney contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by AskRobinhood contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by MTNC_Care contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by mediatemplehelp contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by askvisa contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by OPPOCareIN contain the word 'because' (0.00% of total tweets)\n",
      "0 tweets by HotelTonightCX contain the word 'because' (0.00% of total tweets)\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through each customer care account\n",
    "for account in customer_care_accounts:\n",
    "    # Filter tweets directly involving the account (author or mentioned in text)\n",
    "    account_tweets = df[\n",
    "        (df['author_id'] == account) | \n",
    "        (df['text'].str.contains(f\"@{account}\", case=False, na=False))\n",
    "    ]\n",
    "\n",
    "    # Get all tweet IDs related to the account (including responses and in_response_to)\n",
    "    related_tweet_ids = set(\n",
    "        account_tweets['tweet_id'].tolist() +\n",
    "        [item for sublist in account_tweets['response_tweet_id_list'] if isinstance(sublist, list) for item in sublist] + \n",
    "        [item for sublist in account_tweets['in_response_to_tweet_id_list'] if isinstance(sublist, list) for item in sublist]\n",
    "    )\n",
    "\n",
    "    # Filter all tweets with the related tweet IDs\n",
    "    all_conversation_tweets = df[df['tweet_id'].isin(related_tweet_ids)].copy()  # Create a copy here\n",
    "    \n",
    "    # Count the total number of tweets authored by the account\n",
    "    total_tweets = all_conversation_tweets[all_conversation_tweets['author_id'] == account].shape[0]\n",
    "    \n",
    "    # Count the tweets where the author is the account and \"because\" is in the text\n",
    "    because_count = all_conversation_tweets[\n",
    "        (all_conversation_tweets['author_id'] == account) &\n",
    "        (all_conversation_tweets['text'].str.contains(r'\\bbecause\\b', case=False, na=False))\n",
    "    ].shape[0]\n",
    "    \n",
    "    # Calculate the percentage of tweets containing \"because\"\n",
    "    if total_tweets > 0:\n",
    "        because_percentage = (because_count / total_tweets) * 100\n",
    "    else:\n",
    "        because_percentage = 0  # Handle cases where there are no tweets for the account\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append((account, because_count, because_percentage))\n",
    "\n",
    "# Sort the results by because_percentage in descending order\n",
    "results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the sorted results\n",
    "for account, because_count, because_percentage in results:\n",
    "    print(f\"{because_count} tweets by {account} contain the word 'because' ({because_percentage:.2f}% of total tweets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5e2a301-4c6b-4b14-a9bd-f2fead98cda3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprintcare_merged.csv: 22313 rows\n",
      "Ask_Spectrum_merged.csv: 25776 rows\n",
      "VerizonSupport_merged.csv: 17805 rows\n",
      "ChipotleTweets_merged.csv: 18599 rows\n",
      "AskPlayStation_merged.csv: 18675 rows\n",
      "marksandspencer_merged.csv: 11451 rows\n",
      "MicrosoftHelps_merged.csv: 11216 rows\n",
      "ATVIAssist_merged.csv: 17518 rows\n",
      "AdobeCare_merged.csv: 9876 rows\n",
      "AmazonHelp_merged.csv: 168822 rows\n",
      "XboxSupport_merged.csv: 24312 rows\n",
      "AirbnbHelp_merged.csv: 8821 rows\n",
      "nationalrailenq_merged.csv: 4135 rows\n",
      "AirAsiaSupport_merged.csv: 12724 rows\n",
      "Morrisons_merged.csv: 10021 rows\n",
      "NikeSupport_merged.csv: 3408 rows\n",
      "AskAmex_merged.csv: 11093 rows\n",
      "McDonalds_merged.csv: 8429 rows\n",
      "YahooCare_merged.csv: 875 rows\n",
      "AskLyft_merged.csv: 11787 rows\n",
      "UPSHelp_merged.csv: 17765 rows\n",
      "Delta_merged.csv: 42149 rows\n",
      "AppleSupport_merged.csv: 106648 rows\n",
      "Uber_Support_merged.csv: 56193 rows\n",
      "Tesco_merged.csv: 38470 rows\n",
      "SpotifyCares_merged.csv: 43206 rows\n",
      "British_Airways_merged.csv: 29291 rows\n",
      "comcastcares_merged.csv: 32975 rows\n",
      "AmericanAir_merged.csv: 36531 rows\n",
      "TMobileHelp_merged.csv: 34229 rows\n",
      "VirginTrains_merged.csv: 27470 rows\n",
      "SouthwestAir_merged.csv: 28863 rows\n",
      "AskeBay_merged.csv: 9612 rows\n",
      "hulu_support_merged.csv: 21772 rows\n",
      "GWRHelp_merged.csv: 19270 rows\n",
      "sainsburys_merged.csv: 19399 rows\n",
      "AskPayPal_merged.csv: 11262 rows\n",
      "HPSupport_merged.csv: 7181 rows\n",
      "ChaseSupport_merged.csv: 8708 rows\n",
      "CoxHelp_merged.csv: 7824 rows\n",
      "DropboxSupport_merged.csv: 5940 rows\n",
      "VirginAtlantic_merged.csv: 4292 rows\n",
      "BofA_Help_merged.csv: 12641 rows\n",
      "AzureSupport_merged.csv: 7473 rows\n",
      "AlaskaAir_merged.csv: 7414 rows\n",
      "ArgosHelpers_merged.csv: 12032 rows\n",
      "Postmates_Help_merged.csv: 3336 rows\n",
      "AskTarget_merged.csv: 13210 rows\n",
      "GoDaddyHelp_merged.csv: 1948 rows\n",
      "CenturyLinkHelp_merged.csv: 3795 rows\n",
      "AskPapaJohns_merged.csv: 3893 rows\n",
      "SW_Help_merged.csv: 11775 rows\n",
      "askpanera_merged.csv: 2152 rows\n",
      "Walmart_merged.csv: 2614 rows\n",
      "USCellularCares_merged.csv: 1174 rows\n",
      "AsurionCares_merged.csv: 1847 rows\n",
      "GloCare_merged.csv: 6301 rows\n",
      "idea_cares_merged.csv: 15596 rows\n",
      "DoorDash_Help_merged.csv: 1574 rows\n",
      "NeweggService_merged.csv: 1066 rows\n",
      "VirginAmerica_merged.csv: 2802 rows\n",
      "Safaricom_Care_merged.csv: 15453 rows\n",
      "DunkinDonuts_merged.csv: 1278 rows\n",
      "Ask_WellsFargo_merged.csv: 7559 rows\n",
      "O2_merged.csv: 16079 rows\n",
      "TfL_merged.csv: 2218 rows\n",
      "asksalesforce_merged.csv: 707 rows\n",
      "airtel_care_merged.csv: 9866 rows\n",
      "Kimpton_merged.csv: 1282 rows\n",
      "AskCiti_merged.csv: 4036 rows\n",
      "IHGService_merged.csv: 2625 rows\n",
      "LondonMidland_merged.csv: 6515 rows\n",
      "JetBlue_merged.csv: 7957 rows\n",
      "BoostCare_merged.csv: 904 rows\n",
      "JackBox_merged.csv: 255 rows\n",
      "AldiUK_merged.csv: 7927 rows\n",
      "HiltonHelp_merged.csv: 829 rows\n",
      "GooglePlayMusic_merged.csv: 803 rows\n",
      "OfficeSupport_merged.csv: 214 rows\n",
      "KFC_UKI_Help_merged.csv: 2098 rows\n",
      "DellCares_merged.csv: 5236 rows\n",
      "TwitterSupport_merged.csv: 1277 rows\n",
      "GreggsOfficial_merged.csv: 4493 rows\n",
      "ATT_merged.csv: 2718 rows\n",
      "TacoBellTeam_merged.csv: 4081 rows\n",
      "AskRBC_merged.csv: 1009 rows\n",
      "ArbysCares_merged.csv: 1904 rows\n",
      "NortonSupport_merged.csv: 1989 rows\n",
      "AskSeagate_merged.csv: 638 rows\n",
      "sizehelpteam_merged.csv: 1475 rows\n",
      "SCsupport_merged.csv: 1250 rows\n",
      "MOO_merged.csv: 616 rows\n",
      "AskDSC_merged.csv: 210 rows\n",
      "AskVirginMoney_merged.csv: 906 rows\n",
      "AskRobinhood_merged.csv: 430 rows\n",
      "MTNC_Care_merged.csv: 774 rows\n",
      "AWSSupport_merged.csv: 1034 rows\n",
      "VMUcare_merged.csv: 1350 rows\n",
      "mediatemplehelp_merged.csv: 302 rows\n",
      "AskTigogh_merged.csv: 714 rows\n",
      "PandoraSupport_merged.csv: 1022 rows\n",
      "askvisa_merged.csv: 709 rows\n",
      "OPPOCareIN_merged.csv: 868 rows\n",
      "ask_progressive_merged.csv: 611 rows\n",
      "PearsonSupport_merged.csv: 824 rows\n",
      "CarlsJr_merged.csv: 182 rows\n",
      "HotelTonightCX_merged.csv: 152 rows\n",
      "KeyBank_Help_merged.csv: 552 rows\n",
      "Total files generated: 108\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# put questions and answers side-by-side.\n",
    "# all @ mentions are removed from text as well.\n",
    "#\n",
    "# output location:\n",
    "#        /twcs/by_account2    \n",
    "#\n",
    "# schema:\n",
    "#       in_response_to_tweet_id: this is question from user \n",
    "#       text: this response from customer care account\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Apply the function to extract tweet IDs\n",
    "df['response_tweet_id_list'] = df['response_tweet_id'].apply(extract_tweet_ids)\n",
    "df['in_response_to_tweet_id_list'] = df['in_response_to_tweet_id'].apply(extract_tweet_ids)\n",
    "\n",
    "# Convert in_response_to_tweet_id to integer, with NaN replaced by -1 to avoid issues\n",
    "df['in_response_to_tweet_id'] = pd.to_numeric(df['in_response_to_tweet_id'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "# Merge the DataFrame to create side-by-side question and answer pairs\n",
    "merged_df = pd.merge(\n",
    "    df,\n",
    "    df[['tweet_id', 'text']].rename(columns={'tweet_id': 'in_response_to_tweet_id', 'text': 'in_response_to_tweet_text'}),\n",
    "    on='in_response_to_tweet_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove @ mentions and trailing spaces from both 'in_response_to_tweet_text' and 'text' fields\n",
    "merged_df['in_response_to_tweet_text'] = merged_df['in_response_to_tweet_text'].astype(str).str.replace(r'@[^\\s]+', '', regex=True).str.strip()\n",
    "merged_df['text'] = merged_df['text'].astype(str).str.replace(r'@[^\\s]+', '', regex=True).str.strip()\n",
    "\n",
    "# Remove records where in_response_to_tweet_id is -1 or in_response_to_tweet_text is NaN\n",
    "merged_df = merged_df[(merged_df['in_response_to_tweet_id'] != -1) & (merged_df['in_response_to_tweet_text'] != 'nan')]\n",
    "\n",
    "# Reorder columns for clarity\n",
    "merged_df = merged_df[[\n",
    "    'tweet_id', 'author_id', 'inbound', 'created_at', \n",
    "    'in_response_to_tweet_id', 'in_response_to_tweet_text', \n",
    "    'text', 'response_tweet_id'\n",
    "]]\n",
    "\n",
    "# Filter for customer care accounts (alphanumeric author_ids)\n",
    "customer_care_accounts = merged_df[pd.to_numeric(merged_df['author_id'], errors='coerce').isna()]['author_id'].unique()\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = 'twcs/by_account2/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each customer care account\n",
    "for account in customer_care_accounts:\n",
    "    # Filter tweets involving the account\n",
    "    account_tweets = merged_df[\n",
    "        (merged_df['author_id'] == account)\n",
    "    ]\n",
    "\n",
    "    # Write the merged tweets to a CSV file\n",
    "    output_file = os.path.join(output_dir, f\"{account}_merged.csv\")\n",
    "    account_tweets.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print the number of rows in the output file\n",
    "    print(f\"{account}_merged.csv: {len(account_tweets)} rows\")\n",
    "\n",
    "# Print the total number of files generated\n",
    "print(f\"Total files generated: {len(customer_care_accounts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded60a85-aafc-42d6-bd3e-ec093f164e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-12:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
