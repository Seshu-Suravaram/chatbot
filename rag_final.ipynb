{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b46970-5565-49b4-ae2a-e915e90b6c2f",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Systems, LangChain & ChromaDB\n",
    "\n",
    "This notebook walks through building a question/answer system that retrieves information to formulate responses, effectively grounding the LLM with specific information. A pre-trained LLM, or likely even a fine-tuned LLM will not be sufficient (in and of itself) when you want a system that understands specific, possibly private data or information that was not in its training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4e10c-2fce-444a-a302-e153a68b0a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Select the kernel `langchain_components_kernel` in the top right before going forward in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e8220-6d91-4938-8188-7b3d986b2845",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39d15f9a-3ab7-4d99-994d-d5d235ddee3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd ~/asl-ml-immersion && make langchain_components_kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6855635-3a1c-4295-acb3-999a634db4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "from vertexai.preview import rag\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe53-dee2-4b22-b66a-d43f0b7d6fa4",
   "metadata": {},
   "source": [
    "### Build a simple retrieval augmented generation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "428d7ff0-426f-4c0b-bb58-2a6177088e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_csv_to_train_test(csv_file_path, train_df_name, test_df_name, split_ratio=0.92):\n",
    "    full_df = pd.read_csv(csv_file_path)    \n",
    "    full_df.loc[:, 'text'] = full_df['text'].astype(str)\n",
    "    full_df.loc[:, 'in_response_to_tweet_text'] = full_df['in_response_to_tweet_text'].astype(str)\n",
    "    # Calculate the number of rows for the training set based on the split ratio\n",
    "    train_size = int(split_ratio * len(full_df))\n",
    "    train_df = full_df.iloc[:train_size].copy()\n",
    "    test_df = full_df.iloc[train_size:].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = split_csv_to_train_test(\"twcs/by_account2/ChipotleTweets_merged.csv\", \"train_df\", \"test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7a28f98d-7a59-468a-aa60-3436a6b7343b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "      <th>in_response_to_tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-31 22:14:28+00:00</td>\n",
       "      <td>66</td>\n",
       "      <td>I don't fit in my Veggie Burrito costume #Hall...</td>\n",
       "      <td>I still think you look great! -Becky</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-31 22:14:00+00:00</td>\n",
       "      <td>69</td>\n",
       "      <td>messed up today and didn’t give me my $3 burri...</td>\n",
       "      <td>I'm so sorry about that. Please tell us more s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-31 22:13:29+00:00</td>\n",
       "      <td>71</td>\n",
       "      <td>hey  wanna come to Mammoth. I'll at least eat ...</td>\n",
       "      <td>Hopefully we'll get there at some point! -Becky</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-31 22:12:39+00:00</td>\n",
       "      <td>74</td>\n",
       "      <td>I had excellent service tonight too! Plenty of...</td>\n",
       "      <td>Guac on! I'm happy it was such a great experie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-10-31 20:37:31+00:00</td>\n",
       "      <td>76</td>\n",
       "      <td>When you're the only one in costume #boorito</td>\n",
       "      <td>It's because you're smart. -Tara</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       author_id  inbound                 created_at  \\\n",
       "0        64  ChipotleTweets    False  2017-10-31 22:14:28+00:00   \n",
       "1        68  ChipotleTweets    False  2017-10-31 22:14:00+00:00   \n",
       "2        70  ChipotleTweets    False  2017-10-31 22:13:29+00:00   \n",
       "3        73  ChipotleTweets    False  2017-10-31 22:12:39+00:00   \n",
       "4        75  ChipotleTweets    False  2017-10-31 20:37:31+00:00   \n",
       "\n",
       "   in_response_to_tweet_id                          in_response_to_tweet_text  \\\n",
       "0                       66  I don't fit in my Veggie Burrito costume #Hall...   \n",
       "1                       69  messed up today and didn’t give me my $3 burri...   \n",
       "2                       71  hey  wanna come to Mammoth. I'll at least eat ...   \n",
       "3                       74  I had excellent service tonight too! Plenty of...   \n",
       "4                       76       When you're the only one in costume #boorito   \n",
       "\n",
       "                                                text response_tweet_id  \n",
       "0               I still think you look great! -Becky                65  \n",
       "1  I'm so sorry about that. Please tell us more s...               NaN  \n",
       "2    Hopefully we'll get there at some point! -Becky               NaN  \n",
       "3  Guac on! I'm happy it was such a great experie...               NaN  \n",
       "4                   It's because you're smart. -Tara                74  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the training set\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e3438486-7450-4b47-90bd-390eb548b796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "      <th>in_response_to_tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17111</th>\n",
       "      <td>2803785</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:30:37+00:00</td>\n",
       "      <td>2803787</td>\n",
       "      <td>I ordered  through the app. First time their f...</td>\n",
       "      <td>That's a bummer. Were you able to let a manage...</td>\n",
       "      <td>2803786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17112</th>\n",
       "      <td>2803788</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 02:12:00+00:00</td>\n",
       "      <td>2803786</td>\n",
       "      <td>Normally the place I go is outstanding. I’m ju...</td>\n",
       "      <td>I hope so too. -Tay</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>2803789</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:30:01+00:00</td>\n",
       "      <td>2803791</td>\n",
       "      <td>I haven't had  in months and I can say I reall...</td>\n",
       "      <td>That's no goo. Which location did you go to? -Tay</td>\n",
       "      <td>2803790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>2803792</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 02:05:52+00:00</td>\n",
       "      <td>2803790</td>\n",
       "      <td>239 S. Kings Dr. In Charlotte, NC</td>\n",
       "      <td>Bummer. Please be sure to let a manager know o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17115</th>\n",
       "      <td>2803793</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:26:00+00:00</td>\n",
       "      <td>2803794</td>\n",
       "      <td>I just experience the world record for fastest...</td>\n",
       "      <td>Sounds like a great day to me. -Tay</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id       author_id  inbound                 created_at  \\\n",
       "17111   2803785  ChipotleTweets    False  2017-11-22 01:30:37+00:00   \n",
       "17112   2803788  ChipotleTweets    False  2017-11-22 02:12:00+00:00   \n",
       "17113   2803789  ChipotleTweets    False  2017-11-22 01:30:01+00:00   \n",
       "17114   2803792  ChipotleTweets    False  2017-11-22 02:05:52+00:00   \n",
       "17115   2803793  ChipotleTweets    False  2017-11-22 01:26:00+00:00   \n",
       "\n",
       "       in_response_to_tweet_id  \\\n",
       "17111                  2803787   \n",
       "17112                  2803786   \n",
       "17113                  2803791   \n",
       "17114                  2803790   \n",
       "17115                  2803794   \n",
       "\n",
       "                               in_response_to_tweet_text  \\\n",
       "17111  I ordered  through the app. First time their f...   \n",
       "17112  Normally the place I go is outstanding. I’m ju...   \n",
       "17113  I haven't had  in months and I can say I reall...   \n",
       "17114                  239 S. Kings Dr. In Charlotte, NC   \n",
       "17115  I just experience the world record for fastest...   \n",
       "\n",
       "                                                    text response_tweet_id  \n",
       "17111  That's a bummer. Were you able to let a manage...           2803786  \n",
       "17112                                I hope so too. -Tay               NaN  \n",
       "17113  That's no goo. Which location did you go to? -Tay           2803790  \n",
       "17114  Bummer. Please be sure to let a manager know o...               NaN  \n",
       "17115                Sounds like a great day to me. -Tay               NaN  "
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the test set\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "80224df2-deed-412c-bec8-ba72b8fcc7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   tweet_id                 50000 non-null  int64  \n",
      " 1   author_id                50000 non-null  object \n",
      " 2   inbound                  50000 non-null  bool   \n",
      " 3   created_at               50000 non-null  object \n",
      " 4   text                     50000 non-null  object \n",
      " 5   response_tweet_id        33442 non-null  object \n",
      " 6   in_response_to_tweet_id  37084 non-null  float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(4)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# types of data\n",
    "training_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfca1b4-0600-44fc-909e-730ab4afcd56",
   "metadata": {},
   "source": [
    "# At the core of most retrieval generation systems is a vector database. A vector database stores embedded representations of information.  [Vertex AI text-embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "ff137784-b004-4a96-99db-142f9208571e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split documents into chunks \n",
    "class Document:\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.page_content = content\n",
    "        self.metadata = metadata if metadata is not None else {}\n",
    "        \n",
    "def process_dataframe_in_batches(df, batch_size):\n",
    "    \"\"\"\n",
    "    Process the DataFrame in batches to avoid memory issues and split the text into chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with a column named 'text'.\n",
    "    batch_size (int): The size of each batch to process.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Define the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    # Process the DataFrame in batches\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        # Extract the batch of text and combine into a single string\n",
    "        batch_text = \" \".join(df['in_response_to_tweet_text'][start:end].tolist())\n",
    "        # Wrap the combined text in a Document object\n",
    "        doc = Document(batch_text)\n",
    "        # Split the combined text into chunks\n",
    "        batch_chunks = text_splitter.split_documents([doc])\n",
    "        # Extend the chunks list with the new chunks\n",
    "        chunks.extend(batch_chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Assuming `training_df` is a DataFrame with a column named 'text'\n",
    "batch_size = 1000\n",
    "chunks = process_dataframe_in_batches(train_df, batch_size)\n",
    "\n",
    "# Convert the first two chunks to a DataFrame\n",
    "df_chunks = pd.DataFrame([chunk.page_content for chunk in chunks[0:2]], columns=['Chunk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "9f0092d7-348c-456d-9827-07e398006c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't fit in my Veggie Burrito costume #Hall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/7tJDVpzLWn messed up today and di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Chunk\n",
       "0  I don't fit in my Veggie Burrito costume #Hall...\n",
       "1  https://t.co/7tJDVpzLWn messed up today and di..."
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "df_chunks.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "c31316e9-273f-4ca6-a252-8de9f0e9224e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 50000\n",
      "Number of chunks: 27766\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(training_df)}\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52830b13-f2fe-4a28-a8a3-d529da45757f",
   "metadata": {},
   "source": [
    "#### Embed Document Chunks \n",
    "Now we need to embed the document chunks and store them in a vectorstore. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the PaLM API for Embeddings. The langchain library provides a nice wrapper class around the PaLM Embeddings API, VertexAIEmbeddings().\n",
    "\n",
    "Since Vertex AI Vector Search takes awhile (~45 minutes) to create an index, we will use Chroma instead to keep things simple. Of course, in a real-world use case with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "012aa63d-726d-4393-8293-88776458c40b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n",
      "Gapic client context issue detected.This can occur due to parallelization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e6adfb4e384941a642ee277c5433b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VertexAIEmbeddings.update_forward_refs()\n",
    "embeddings = VertexAIEmbeddings(\"text-embedding-004\")\n",
    "# set persist directory so the vector store is saved to disk\n",
    "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f28527-2b38-4c51-a739-b56e499383c9",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and PaLM as our LLM. Langchain provides a wrapper around PaLM, `VertexAI()`.\n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "0c1b4f33-0fe2-46e8-a898-ade9e26e7510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 20} # number of nearest neighbors to retrieve\n",
    ")\n",
    "\n",
    "\n",
    "retriever_gemini = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 50} # number of nearest neighbors to retrieve\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0811-c32c-4059-a7e0-8f8771dd68d2",
   "metadata": {},
   "source": [
    "Now that everything is tied together we can send queries and get answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "1e47824e-2d8a-45d1-9fa0-3b87092b217a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################### prompt #########################################################\n",
    "\n",
    "def get_prompt(query: str):\n",
    "    print(query)\n",
    "    prompt = f\"\"\"\n",
    "    Using only the provided context, answer the question.\n",
    "    \n",
    "    You are a helpful customer service representative online on twitter for a fastfood chain Chipotle which serves mexican fastfood in the Tex-Mex style. You are tasked with providing appropriate response to the user based on type of outreach from user. Here is probable list of categories for the customer tweet and guidelines on how you should respond:\n",
    "        1.question - answer only based on provided context .If you need more info, ask for it.\n",
    "        2.complaint - apologize and explain the situation\n",
    "        3.appreciation - Give a warm thank you!\n",
    "        4.joke or wisecracks - Keep it fun and light-hearted, and thank the customer\n",
    "       \n",
    "        Ensure that your response fits within Twitter's character limit (120 characters).\n",
    "        If a tweet falls into multiple categories, use a combination of the respective guidelines.\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    If you cannot answer the question using only the provided context, respond that you do not have the context needed to answer the question.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "######################Chain####################################\n",
    "\n",
    "palm = VertexAI(model_name=\"text-bison@001\", temperature=0.1, top_p=0.95,top_k=5,  max_output_tokens=1024)\n",
    "\n",
    "retriver_context_palm_qa = RetrievalQA.from_chain_type(\n",
    "    llm=palm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# gemini API\n",
    "gemini_model = VertexAI(mode_name=\"gemini-1.0-pro\")\n",
    "\n",
    "retriver_context_gemini_llm = RetrievalQA.from_chain_type(\n",
    "    llm=gemini_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever_gemini,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "\n",
    "######################Invoke gemini & palm model ####################################\n",
    "# query gemini model \n",
    "def ask_gemini(question: str):\n",
    "    prompt = get_prompt(question)\n",
    "    \n",
    "    retriever_response = retriver_context_gemini_llm({\"query\": prompt})\n",
    "    \n",
    "    final_response = f\"{retriever_response['result']}\"\n",
    "    # print(retriever_response)\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "# query palm model \n",
    "def ask_palm_question(question: str):    \n",
    "    prompt = get_prompt(question)\n",
    "    # print(\"len of prompt\" , len(prompt))\n",
    "    \n",
    "    response = retriver_context_palm_qa({\"query\": prompt})\n",
    "    # print(response['result'])\n",
    "    final_palm_response = response['result']\n",
    "    \n",
    "    return final_palm_response\n",
    "\n",
    "\n",
    "######################Invoke gemini & palm response ####################################\n",
    "# gemini reponse \n",
    "def get_gemini_response(response_tweet_id):\n",
    "        query = test_df.loc[test_df['tweet_id'] == response_tweet_id, 'in_response_to_tweet_text'].values[0]\n",
    "        response = ask_gemini(query)\n",
    "        print(response)\n",
    "        return response \n",
    "\n",
    "# plam response\n",
    "def get_palm_response(response_tweet_id):\n",
    "    try:\n",
    "        query = test_df.loc[test_df['tweet_id'] == response_tweet_id, 'in_response_to_tweet_text'].values[0]\n",
    "        # print(f\"Tweet ID : {response_tweet_id}\")\n",
    "        response = ask_palm_question(query)\n",
    "        print(response)\n",
    "    except Exception as error:\n",
    "        print(f\"Error occurred: {error}\")\n",
    "        print(f\"Query that caused the error: {query if 'query' in locals() else 'N/A'}\")\n",
    "        return response \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "50acb721-970d-4311-87d3-cb69b7c9ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where’s the Chorizo'"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_df.loc[test_df['tweet_id'] == 2814766.0, 'in_response_to_tweet_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "a383490f-e8f1-4fd2-b31f-c8b4ddb1c007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.32340425531915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df['in_response_to_tweet_text'].describe()\n",
    "\n",
    "lengths = test_df['in_response_to_tweet_text'].astype(str).apply(len)\n",
    "# print((lengths.mean()))\n",
    "\n",
    "# test_df['in_response_to_tweet_text'].astype(str).apply(lambda x: x.str.len().mean())\n",
    "\n",
    "# average_lengths = test_df['in_response_to_tweet_text'].apply(lambda x: x.str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "1e2d3daf-90d9-44e8-83c6-a14c0a49139f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why do we pay extra for Guacamole ???\n",
      "why do we pay extra for Guacamole ???\n",
      "Guacamole is a premium ingredient and we want to make sure that we are able to provide the freshest and highest quality guacamole to our customers.\n",
      "None\n",
      "why do we pay extra for Guacamole ???\n",
      " I'm sorry, but the provided context does not contain the answer to why Chipotle charges extra for guacamole.\n",
      " I'm sorry, but the provided context does not contain the answer to why Chipotle charges extra for guacamole.\n"
     ]
    }
   ],
   "source": [
    "print(test_df.loc[test_df['tweet_id'] == 2814766.0, 'in_response_to_tweet_text'].values[0])\n",
    "print(get_palm_response(2814766.0))\n",
    "print(get_gemini_response(2814766.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "edd505f6-cb5f-42df-9efa-a0e2635596d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function to populate the human_response_tweet_text column\n",
    "test_df['palm_response'] = test_df['tweet_id'].apply(get_palm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "2653cfaa-fa3e-403e-a785-5cbdfca5b2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Apply the function to populate the human_response_tweet_text column\n",
    "test_df['gemini_llm_response'] = test_df['tweet_id'].apply(get_gemini_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "48e13d7d-3642-4f7a-a91d-dca09b548b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1880 entries, 17111 to 391\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   tweet_id                   1488 non-null   float64\n",
      " 1   author_id                  1488 non-null   object \n",
      " 2   inbound                    1488 non-null   object \n",
      " 3   created_at                 1488 non-null   object \n",
      " 4   in_response_to_tweet_id    1488 non-null   float64\n",
      " 5   in_response_to_tweet_text  1488 non-null   object \n",
      " 6   text                       1488 non-null   object \n",
      " 7   response_tweet_id          433 non-null    object \n",
      " 8   palm_response              1488 non-null   object \n",
      " 9   gemini_llm_response        1488 non-null   object \n",
      " 10  palm_score                 314 non-null    float64\n",
      " 11  palm_justifcation          314 non-null    object \n",
      " 12  gemini_1_score             392 non-null    float64\n",
      " 13  gemini_justifcation        392 non-null    float64\n",
      "dtypes: float64(5), object(9)\n",
      "memory usage: 284.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "a13f62b6-c3d4-48dd-a8ec-a1ee2559ff07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(\"llm_responses.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "f4e6aa5a-bbf0-4c44-91bd-41b6d398d7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create prompt template \n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template_temp = \"\"\"\n",
    "You are a customer service Manager on twitter for a fastfood chain Chipotle which serves mexican fastfood in the Tex-Mex style  \n",
    "\n",
    "    1. Understandability: response is  easy to understand, any technical terms or jargon are explained adequately.Ignore\n",
    "    2. Actionability: offer potential solutions, next steps for the customer,  assurances that the issue will be addressed or gather more information\n",
    "    \n",
    "\n",
    "GIVEN ANSWER: {llm_response}\n",
    "TRUE ANSWER: {human_response}\n",
    "\n",
    "Example 1: \n",
    "  Text: 129 Kings street, MountainView CA, \n",
    "  Answer is : The provided context does not mention anything about 129 Kings street, MountainView CA, , so I cannot answer this question.\n",
    "  score is : 2\n",
    "  \n",
    "Example 2: \n",
    "  Text: The rice was probably undercooked\"\n",
    "  Answer is :  I'm sorry to hear that your rice was crunchy. Unfortunately, I can't find any information in the provided text about what can make rice less watery and have a richer flavor.\n",
    "  score is : 1\n",
    "  \n",
    "\n",
    "SCORE: Assign a single score of 0, 1, or 2 to the given answer.  Evaluate which response is more effective in terms of engaging with the user and addressing their concerns comprehensively.\n",
    "\n",
    "JUSTIFICATION: (Provide reasoning for your score and explain why the GIVEN answer is scored the way you did, referencing the criteria and examples)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "   you are grading the similarity between two customer service agents. \n",
    "   The responses can be worded differently, but evaluate if they are similar to each other. \n",
    "   Check if they address the customer's issue.\n",
    "   The tone, approach and level of problem-solving can vary between the response.\n",
    "    \n",
    "    \n",
    "    Customer issue : {query}\n",
    "    Customer service agents are below: \n",
    "    Customer Service agent 1: {human_response}\n",
    "    Customer Service agent 2: {llm_response}\n",
    "    \n",
    "    \n",
    "    If customer service agent 2's response is not a valid response to the customer issue, the score is 0.\n",
    "    If customer service agent 2's response, varies from customer service agent 1's response but varies in tone, approach, wording and level of problem solving, the score is 1.\n",
    "    \n",
    "    SCORE: (Provide a score of 0 or 1 only)\n",
    "    \n",
    "   JUSTIFICATION: (Provide reasoning for your score)\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\",\"llm_response\", \"human_response\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "def create_formatted_prompt(query,llm_response, human_response):\n",
    "    # Format the prompt with the given responses\n",
    "    formatted_prompt = prompt.format(query= query,\n",
    "        llm_response=llm_response,\n",
    "        human_response=human_response\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "c6ad10b3-fbc9-4044-9a9d-b8cea157f68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_score_and_justification(text_response):\n",
    "    \"\"\"\n",
    "    Extracts the score and justification from the text response.\n",
    "\n",
    "    Parameters:\n",
    "    - text_response (str): The response text from the model which includes the score and justification.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (score (int), justification (str))\n",
    "    \"\"\"\n",
    "    # Regex to find the score in the format \"SCORE: X\" where X is a number\n",
    "    print(text_response)\n",
    "    score_match = re.search(r'**SCORE:**\\s*(\\d+)', text_response)\n",
    "    # Extract the score if found\n",
    "    score = int(score_match.group(1)) if score_match else 0\n",
    "\n",
    "    # Extract the justification text following the \"JUSTIFICATION:\" label\n",
    "    justification_match = re.search(r'**JUSTIFICATION**:\\s(.*)', text_response, re.DOTALL)\n",
    "    # Extract the justification if found\n",
    "    justification = justification_match.group(1).strip() if justification_match else 0\n",
    "\n",
    "    return score, justification\n",
    "\n",
    "\n",
    "# invoke gemini for judgement\n",
    "def evaluate_response(tweet_id,query,llm_response, human_response, model_name=\"gemini-1.5-pro\",\n",
    "    temperature=0.1,max_output_tokens=256, top_p=0.8,\n",
    "                      top_k=5,):\n",
    "    \"\"\"\n",
    "    Evaluates a given response against a human-provided response using a generative model.\n",
    "\n",
    "    Parameters:\n",
    "    - llm_response (str): The response generated by the language model.\n",
    "    - human_response (str): The reference response provided by a human expert.\n",
    "    - model_name (str): The name of the generative model to use.\n",
    "    - temperature (float): Sampling temperature for response generation.\n",
    "    - max_output_tokens (int): Maximum number of tokens in the generated response.\n",
    "    - top_p (float): Nucleus sampling probability.\n",
    "    - top_k (int): Top-k sampling value.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (score (int), justification (str))\n",
    "    \"\"\"\n",
    "        \n",
    "    # format prompt \n",
    "    prompt_final = create_formatted_prompt(query, llm_response, human_response)\n",
    "    # intialize model \n",
    "    # print(prompt_final)\n",
    "    print(query,llm_response, human_response)\n",
    "    print(\"Tweet ID:\",tweet_id)\n",
    "    model = GenerativeModel(model_name)\n",
    "    # generative config\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=1024,\n",
    "        top_p=0.5,\n",
    "        top_k=5\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        prompt_final,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    # print(responses.candidates[0].content.parts[0].text)\n",
    "    print(responses)\n",
    "    score, justification = extract_score_and_justification(responses.candidates[0].content.parts[0].text)\n",
    "\n",
    "    return score, justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6beb60b5-b408-4045-bb89-46e71cda09c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# responses = evaluate_response( test_tweet_data_df['llm_response'].loc[10] , test_tweet_data_df['human_response_tweet_text'].loc[10])\n",
    "# # print(responses.candidates[0].content.parts[0].text)\n",
    "# # print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "c6b52484-1b13-4684-b54c-6f81435dbedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normally the place I go is outstanding. I’m just hoping this is a fluke. I'm sorry to hear that you had a bad experience. We're always looking for ways to improve our service, so please let us know what happened so we can make it right.\n",
      " I hope so too. -Tay\n",
      "Tweet ID: 2803788.0\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"**SCORE:** 1\\n\\n**JUSTIFICATION:** Customer service agent 2\\'s response is a valid response to the customer\\'s issue. It acknowledges the customer\\'s negative experience and shows a willingness to address the situation. \\n\\nWhile both agents respond to the customer, they do so with different tones, approaches, wording, and levels of problem-solving. \\n\\n* **Agent 1** offers a simple, empathetic response but doesn\\'t probe for more information or offer solutions. \\n* **Agent 2** takes a more proactive approach, expressing apology, encouraging feedback, and promising action to make things right. \\n \\nTherefore, the score is 1. \\n\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.07177734375\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.042724609375\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0140380859375\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.012451171875\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.08154296875\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0284423828125\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.033203125\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.03515625\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 268\n",
      "  candidates_token_count: 142\n",
      "  total_token_count: 410\n",
      "}\n",
      "\n",
      "**SCORE:** 1\n",
      "\n",
      "**JUSTIFICATION:** Customer service agent 2's response is a valid response to the customer's issue. It acknowledges the customer's negative experience and shows a willingness to address the situation. \n",
      "\n",
      "While both agents respond to the customer, they do so with different tones, approaches, wording, and levels of problem-solving. \n",
      "\n",
      "* **Agent 1** offers a simple, empathetic response but doesn't probe for more information or offer solutions. \n",
      "* **Agent 2** takes a more proactive approach, expressing apology, encouraging feedback, and promising action to make things right. \n",
      " \n",
      "Therefore, the score is 1. \n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[833], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m row \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(row)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m palm_score, palm_justification \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_response_to_tweet_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpalm_response\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuman_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(f\"Row: {i} , Score: {palm_score} , Justificaton:  {palm_justification}\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpalm_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[820], line 67\u001b[0m, in \u001b[0;36mevaluate_response\u001b[0;34m(tweet_id, query, llm_response, human_response, model_name, temperature, max_output_tokens, top_p, top_k)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(responses.candidates[0].content.parts[0].text)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(responses)\n\u001b[0;32m---> 67\u001b[0m score, justification \u001b[38;5;241m=\u001b[39m \u001b[43mextract_score_and_justification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score, justification\n",
      "Cell \u001b[0;32mIn[820], line 15\u001b[0m, in \u001b[0;36mextract_score_and_justification\u001b[0;34m(text_response)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Regex to find the score in the format \"SCORE: X\" where X is a number\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_response)\n\u001b[0;32m---> 15\u001b[0m score_match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m**SCORE:**\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract the score if found\u001b[39;00m\n\u001b[1;32m     17\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(score_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m score_match \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:200\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[1;32m    787\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m--> 788\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    952\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/sre_parse.py:669\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    667\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item \u001b[38;5;129;01mor\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m AT:\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnothing to repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    670\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _REPEATCODES:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    673\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m here \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(this))\n",
      "\u001b[0;31merror\u001b[0m: nothing to repeat at position 0"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row in the DataFrame  palm llm\n",
    "import time \n",
    "# Number of rows to apply the function to\n",
    "num_rows = 8\n",
    "\n",
    "# Apply the function to the first few rows\n",
    "for i in range(1,2):\n",
    "    row = test_df.iloc[i]\n",
    "    # print(row)\n",
    "    palm_score, palm_justification = evaluate_response(tweet_id=row['tweet_id'], query=row['in_response_to_tweet_text'], llm_response= row['palm_response'], human_response=row['text'])\n",
    "    # print(f\"Row: {i} , Score: {palm_score} , Justificaton:  {palm_justification}\")\n",
    "    print(f\"Row: {i} , Score: {palm_score} \")\n",
    "    # test_df.at[i, 'palm_score'] = palm_score\n",
    "    # test_df.at[i, 'palm_justifcation'] = palm_justification\n",
    "    test_df.iloc[i, test_df.columns.get_loc('palm_score')] = palm_score\n",
    "    test_df.iloc[i, test_df.columns.get_loc('palm_justifcation')] = palm_justification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "10809ece-5c8e-4f8b-a9f5-a0a55e1f8741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1880 entries, 17111 to 391\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   tweet_id                   1488 non-null   float64\n",
      " 1   author_id                  1488 non-null   object \n",
      " 2   inbound                    1488 non-null   object \n",
      " 3   created_at                 1488 non-null   object \n",
      " 4   in_response_to_tweet_id    1488 non-null   float64\n",
      " 5   in_response_to_tweet_text  1488 non-null   object \n",
      " 6   text                       1488 non-null   object \n",
      " 7   response_tweet_id          433 non-null    object \n",
      " 8   palm_response              1488 non-null   object \n",
      " 9   gemini_llm_response        1488 non-null   object \n",
      " 10  palm_score                 343 non-null    float64\n",
      " 11  palm_justifcation          343 non-null    object \n",
      " 12  gemini_1_score             392 non-null    float64\n",
      " 13  gemini_justifcation        392 non-null    float64\n",
      "dtypes: float64(5), object(9)\n",
      "memory usage: 284.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# test_tweet_data_df.drop('palm_justification', axis=1, inplace=True)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "1c9ba927-634a-4605-abf6-17be4471d955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of rows to apply the function to\n",
    "\n",
    "# Apply the function to the first few rows\n",
    "for i in range(100,200):\n",
    "    row = test_df.iloc[i]\n",
    "    gemini_1_score, gemini_justifcation = evaluate_response(tweet_id=row['tweet_id'], query=row['in_response_to_tweet_text'], llm_response= row['gemini_llm_response'], human_response=row['text'])\n",
    "    print(f\"Row: {i} , Score: {gemini_1_score}\")\n",
    "    test_df.iloc[i, test_df.columns.get_loc('gemini_1_score')] = gemini_1_score\n",
    "    test_df.iloc[i, test_df.columns.get_loc('gemini_justifcation')] = gemini_justifcation\n",
    "    # test_df.at[i, 'gemini_1_score'] = gemini_1_score\n",
    "    # test_df.at[i, 'gemini_justifcation'] = gemini_justifcation\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "eb07f927-89b0-4965-9ff8-a62d1e5117be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "      <th>in_response_to_tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>palm_response</th>\n",
       "      <th>gemini_llm_response</th>\n",
       "      <th>palm_score</th>\n",
       "      <th>palm_justifcation</th>\n",
       "      <th>gemini_1_score</th>\n",
       "      <th>gemini_justifcation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17111</th>\n",
       "      <td>2803785.0</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:30:37+00:00</td>\n",
       "      <td>2803787.0</td>\n",
       "      <td>I ordered  through the app. First time their f...</td>\n",
       "      <td>That's a bummer. Were you able to let a manage...</td>\n",
       "      <td>2803786</td>\n",
       "      <td>I'm sorry to hear that your food was cold. We ...</td>\n",
       "      <td>I'm sorry to hear that your food was cold. Un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17112</th>\n",
       "      <td>2803788.0</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 02:12:00+00:00</td>\n",
       "      <td>2803786.0</td>\n",
       "      <td>Normally the place I go is outstanding. I’m ju...</td>\n",
       "      <td>I hope so too. -Tay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm sorry to hear that you had a bad experienc...</td>\n",
       "      <td>I'm sorry, but the provided text does not men...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>2803789.0</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:30:01+00:00</td>\n",
       "      <td>2803791.0</td>\n",
       "      <td>I haven't had  in months and I can say I reall...</td>\n",
       "      <td>That's no goo. Which location did you go to? -Tay</td>\n",
       "      <td>2803790</td>\n",
       "      <td>The food was not fresh and the meat was not co...</td>\n",
       "      <td>The user did not state what restaurant they w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>2803792.0</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 02:05:52+00:00</td>\n",
       "      <td>2803790.0</td>\n",
       "      <td>239 S. Kings Dr. In Charlotte, NC</td>\n",
       "      <td>Bummer. Please be sure to let a manager know o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know.\\n</td>\n",
       "      <td>The provided context does not mention anythin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17115</th>\n",
       "      <td>2803793.0</td>\n",
       "      <td>ChipotleTweets</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-22 01:26:00+00:00</td>\n",
       "      <td>2803794.0</td>\n",
       "      <td>I just experience the world record for fastest...</td>\n",
       "      <td>Sounds like a great day to me. -Tay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The answer is \"I just experience the world rec...</td>\n",
       "      <td>I'm sorry, but the provided text does not con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id       author_id inbound                 created_at  \\\n",
       "17111  2803785.0  ChipotleTweets   False  2017-11-22 01:30:37+00:00   \n",
       "17112  2803788.0  ChipotleTweets   False  2017-11-22 02:12:00+00:00   \n",
       "17113  2803789.0  ChipotleTweets   False  2017-11-22 01:30:01+00:00   \n",
       "17114  2803792.0  ChipotleTweets   False  2017-11-22 02:05:52+00:00   \n",
       "17115  2803793.0  ChipotleTweets   False  2017-11-22 01:26:00+00:00   \n",
       "\n",
       "       in_response_to_tweet_id  \\\n",
       "17111                2803787.0   \n",
       "17112                2803786.0   \n",
       "17113                2803791.0   \n",
       "17114                2803790.0   \n",
       "17115                2803794.0   \n",
       "\n",
       "                               in_response_to_tweet_text  \\\n",
       "17111  I ordered  through the app. First time their f...   \n",
       "17112  Normally the place I go is outstanding. I’m ju...   \n",
       "17113  I haven't had  in months and I can say I reall...   \n",
       "17114                  239 S. Kings Dr. In Charlotte, NC   \n",
       "17115  I just experience the world record for fastest...   \n",
       "\n",
       "                                                    text response_tweet_id  \\\n",
       "17111  That's a bummer. Were you able to let a manage...           2803786   \n",
       "17112                                I hope so too. -Tay               NaN   \n",
       "17113  That's no goo. Which location did you go to? -Tay           2803790   \n",
       "17114  Bummer. Please be sure to let a manager know o...               NaN   \n",
       "17115                Sounds like a great day to me. -Tay               NaN   \n",
       "\n",
       "                                           palm_response  \\\n",
       "17111  I'm sorry to hear that your food was cold. We ...   \n",
       "17112  I'm sorry to hear that you had a bad experienc...   \n",
       "17113  The food was not fresh and the meat was not co...   \n",
       "17114                                    I don't know.\\n   \n",
       "17115  The answer is \"I just experience the world rec...   \n",
       "\n",
       "                                     gemini_llm_response  palm_score  \\\n",
       "17111   I'm sorry to hear that your food was cold. Un...         NaN   \n",
       "17112   I'm sorry, but the provided text does not men...         NaN   \n",
       "17113   The user did not state what restaurant they w...         NaN   \n",
       "17114   The provided context does not mention anythin...         NaN   \n",
       "17115   I'm sorry, but the provided text does not con...         NaN   \n",
       "\n",
       "      palm_justifcation  gemini_1_score  gemini_justifcation  \n",
       "17111               NaN             0.0                  0.0  \n",
       "17112               NaN             0.0                  0.0  \n",
       "17113               NaN             NaN                  NaN  \n",
       "17114               NaN             NaN                  NaN  \n",
       "17115               NaN             NaN                  NaN  "
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "7aa936bb-732c-4527-add0-fa882923a683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluvation_df = test_df[['gemini_1_score', 'palm_score']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "b6a89f8b-6531-4b75-a0a9-a6db18a313d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini_1_score    10.0\n",
      "palm_score         3.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "evaluvation_df.head(100)\n",
    "average_values = evaluvation_df.mean()\n",
    "total_rows = len(evaluvation_df)\n",
    "average_values = (evaluvation_df.sum() / (total_rows)) * 100\n",
    "print(average_values)\n",
    "\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Model Name': average_values.index,\n",
    "    'Accuracy (%)': average_values.values\n",
    "})\n",
    "\n",
    "accuracy_df.to_csv(\"model_accuracy.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "langchain_components_kernel",
   "name": "tf2-gpu.2-12.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-12:m123"
  },
  "kernelspec": {
   "display_name": "langchain_components_kernel (Local)",
   "language": "python",
   "name": "langchain_components_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
